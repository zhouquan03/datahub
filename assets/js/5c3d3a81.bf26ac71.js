"use strict";(self.webpackChunkdocs_website=self.webpackChunkdocs_website||[]).push([[2367],{4137:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return u}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),m=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=m(e.components);return n.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),c=m(a),u=r,h=c["".concat(l,".").concat(u)]||c[u]||p[u]||i;return a?n.createElement(h,o(o({ref:t},d),{},{components:a})):n.createElement(h,o({ref:t},d))}));function u(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var m=2;m<i;m++)o[m]=a[m];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},9850:function(e,t,a){a.r(t),a.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return m},toc:function(){return d},default:function(){return c}});var n=a(7462),r=a(3366),i=(a(7294),a(4137)),o=["components"],s={title:"Python Emitter",sidebar_label:"Python Emitter",slug:"/metadata-ingestion/as-a-library",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/metadata-ingestion/as-a-library.md"},l="Python Emitter",m={unversionedId:"metadata-ingestion/as-a-library",id:"metadata-ingestion/as-a-library",isDocsHomePage:!1,title:"Python Emitter",description:"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc.",source:"@site/genDocs/metadata-ingestion/as-a-library.md",sourceDirName:"metadata-ingestion",slug:"/metadata-ingestion/as-a-library",permalink:"/docs/metadata-ingestion/as-a-library",editUrl:"https://github.com/linkedin/datahub/blob/master/metadata-ingestion/as-a-library.md",tags:[],version:"current",frontMatter:{title:"Python Emitter",sidebar_label:"Python Emitter",slug:"/metadata-ingestion/as-a-library",custom_edit_url:"https://github.com/linkedin/datahub/blob/master/metadata-ingestion/as-a-library.md"},sidebar:"overviewSidebar",previous:{title:"File",permalink:"/docs/metadata-ingestion/sink_docs/file"},next:{title:"Java Emitter",permalink:"/docs/metadata-integration/java/as-a-library"}},d=[{value:"Installation",id:"installation",children:[],level:2},{value:"REST Emitter",id:"rest-emitter",children:[{value:"Installation",id:"installation-1",children:[],level:3},{value:"Example Usage",id:"example-usage",children:[],level:3},{value:"Emitter Code",id:"emitter-code",children:[],level:3}],level:2},{value:"Kafka Emitter",id:"kafka-emitter",children:[{value:"Installation",id:"installation-2",children:[],level:3},{value:"Example Usage",id:"example-usage-1",children:[],level:3},{value:"Emitter Code",id:"emitter-code-1",children:[],level:3}],level:2},{value:"Other Languages",id:"other-languages",children:[],level:2}],p={toc:d};function c(e){var t=e.components,a=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},p,a,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"python-emitter"},"Python Emitter"),(0,i.kt)("p",null,"In some cases, you might want to construct Metadata events directly and use programmatic ways to emit that metadata to DataHub. Use-cases are typically push-based and include emitting metadata events from CI/CD pipelines, custom orchestrators etc. "),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"acryl-datahub")," Python package offers REST and Kafka emitter API-s, which can easily be imported and called from your own code."),(0,i.kt)("h2",{id:"installation"},"Installation"),(0,i.kt)("p",null,"Follow the installation guide for the main ",(0,i.kt)("inlineCode",{parentName:"p"},"acryl-datahub")," package ",(0,i.kt)("a",{parentName:"p",href:"/docs/metadata-ingestion#install-from-pypi"},"here"),". Read on for emitter specific installation instructions."),(0,i.kt)("h2",{id:"rest-emitter"},"REST Emitter"),(0,i.kt)("p",null,"The REST emitter is a thin wrapper on top of the ",(0,i.kt)("inlineCode",{parentName:"p"},"requests")," module and offers a blocking interface for sending metadata events over HTTP. Use this when simplicity and acknowledgement of metadata being persisted to DataHub's metadata store is more important than throughput of metadata emission. Also use this when read-after-write scenarios exist, e.g. writing metadata and then immediately reading it back."),(0,i.kt)("h3",{id:"installation-1"},"Installation"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"pip install -U `acryl-datahub[datahub-rest]`\n")),(0,i.kt)("h3",{id:"example-usage"},"Example Usage"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import datahub.emitter.mce_builder as builder\nfrom datahub.emitter.mcp import MetadataChangeProposalWrapper\nfrom datahub.metadata.schema_classes import ChangeTypeClass, DatasetPropertiesClass\n\nfrom datahub.emitter.rest_emitter import DatahubRestEmitter\n\n# Create an emitter to DataHub over REST\nemitter = DatahubRestEmitter(gms_server="http://localhost:8080", extra_headers={})\n\n# Test the connection\nemitter.test_connection()\n\n# Construct a dataset properties object\ndataset_properties = DatasetPropertiesClass(description="This table stored the canonical User profile",\n    customProperties={\n         "governance": "ENABLED"\n    })\n\n# Construct a MetadataChangeProposalWrapper object.\nmetadata_event = MetadataChangeProposalWrapper(\n    entityType="dataset",\n    changeType=ChangeTypeClass.UPSERT,\n    entityUrn=builder.make_dataset_urn("bigquery", "my-project.my-dataset.user-table"),\n    aspectName="datasetProperties",\n    aspect=dataset_properties,\n)\n\n# Emit metadata! This is a blocking call\nemitter.emit(metadata_event)\n')),(0,i.kt)("p",null,"Other examples:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"https://github.com/linkedin/datahub/blob/master/metadata-ingestion/examples/library/lineage_emitter_mcpw_rest.py"},"lineage_emitter_mcpw_rest.py")," - emits simple bigquery table-to-table (dataset-to-dataset) lineage via REST as MetadataChangeProposalWrapper.")),(0,i.kt)("h3",{id:"emitter-code"},"Emitter Code"),(0,i.kt)("p",null,"If you're interested in looking at the REST emitter code, it is available ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-ingestion/src/datahub/emitter/rest_emitter.py"},"here")),(0,i.kt)("h2",{id:"kafka-emitter"},"Kafka Emitter"),(0,i.kt)("p",null,"The Kafka emitter is a thin wrapper on top of the SerializingProducer class from ",(0,i.kt)("inlineCode",{parentName:"p"},"confluent-kafka")," and offers a non-blocking interface for sending metadata events to DataHub. Use this when you want to decouple your metadata producer from the uptime of your datahub metadata server by utilizing Kafka as a highly available message bus. For example, if your DataHub metadata service is down due to planned or unplanned outages, you can still continue to collect metadata from your mission critical systems by sending it to Kafka. Also use this emitter when throughput of metadata emission is more important than acknowledgement of metadata being persisted to DataHub's backend store."),(0,i.kt)("p",null,(0,i.kt)("strong",{parentName:"p"},(0,i.kt)("em",{parentName:"strong"},"Note")),": The Kafka emitter uses Avro to serialize the Metadata events to Kafka. Changing the serializer will result in unprocessable events as DataHub currently expects the metadata events over Kafka to be serialized in Avro."),(0,i.kt)("h3",{id:"installation-2"},"Installation"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"# For emission over Kafka\npip install -U `acryl-datahub[datahub-kafka]`\n")),(0,i.kt)("h3",{id:"example-usage-1"},"Example Usage"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'import datahub.emitter.mce_builder as builder\nfrom datahub.emitter.mcp import MetadataChangeProposalWrapper\nfrom datahub.metadata.schema_classes import ChangeTypeClass, DatasetPropertiesClass\n\nfrom datahub.emitter.kafka_emitter import DatahubKafkaEmitter, KafkaEmitterConfig\n# Create an emitter to Kafka\nkafka_config = {\n    "connection": {\n        "bootstrap": "localhost:9092",\n        "schema_registry_url": "http://localhost:8081",\n        "schema_registry_config": {}, # schema_registry configs passed to underlying schema registry client\n        "producer_config": {}, # extra producer configs passed to underlying kafka producer\n    }\n}\n\nemitter = DatahubKafkaEmitter(\n    KafkaEmitterConfig.parse_obj(kafka_config)\n)\n\n# Construct a dataset properties object\ndataset_properties = DatasetPropertiesClass(description="This table stored the canonical User profile",\n    customProperties={\n         "governance": "ENABLED"\n    })\n\n# Construct a MetadataChangeProposalWrapper object.\nmetadata_event = MetadataChangeProposalWrapper(\n    entityType="dataset",\n    changeType=ChangeTypeClass.UPSERT,\n    entityUrn=builder.make_dataset_urn("bigquery", "my-project.my-dataset.user-table"),\n    aspectName="datasetProperties",\n    aspect=dataset_properties,\n)\n\n\n# Emit metadata! This is a non-blocking call\nemitter.emit(\n    metadata_event,\n    callback=lambda exc, message: print(f"Message sent to topic:{message.topic()}, partition:{message.partition()}, offset:{message.offset()}") if message else print(f"Failed to send with: {exc}")\n)\n\n#Send all pending events\nemitter.flush()\n')),(0,i.kt)("h3",{id:"emitter-code-1"},"Emitter Code"),(0,i.kt)("p",null,"If you're interested in looking at the Kafka emitter code, it is available ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/linkedin/datahub/blob/master/metadata-ingestion/src/datahub/emitter/kafka_emitter.py"},"here")),(0,i.kt)("h2",{id:"other-languages"},"Other Languages"),(0,i.kt)("p",null,"Emitter API-s are also supported for:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("a",{parentName:"li",href:"/docs/metadata-integration/java/as-a-library"},"Java"))))}c.isMDXComponent=!0}}]);